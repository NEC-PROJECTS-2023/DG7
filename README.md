# DG7
Abstractive Text Summarization Using BART Model
Presented By:

  P.Gayathri (19471A05N6)
  M.Naga Sirisha (20475A0505)
  Ch.Dhanalakshmi (19471A05K3)
  
Introduction:
A new abstractive summarization model for documents, hierarchical BART (HieBART), which captures the hierarchical structures of documents (i.e., their sentence-word
structures) in the BART model. Although the existing BART model has achieved state-of-theart performance on document summarization tasks, it does not account for interactions between sentence-level and word-level information. In machine translation tasks, the performance of neural machine translation models can be improved with the incorporation of multi-granularity self-attention (MG-SA), which captures relationships between words and phrases. Inspired by previous work, the proposed HieBART model incorporates MG-SA into the encoder of the BART model for capturing sentence-word structures. Evaluations performed on the CNN/Daily Mail dataset show that the proposed Hie-BART model outperforms strong baselines and improves the performance of a non-hierarchical BART model.
Dataset Link: https://www.kaggle.com/datasets/gowrishankarp/newspaper-text-summarization-cnn-dailymail
